---
title: "Functions and Iteration"
author: "Slava Nikitin"
date: "2017-04-03"
abstract: ""
abstract_short: ""
authors: []
image: ""
image_preview: ""
math: true
publication_types: []
publication: ""
publication_short: ""
selected: false
url_code: ""
url_dataset: "https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/download/train.json.zip"
url_pdf: ""
url_project: "https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/data"
url_slides: ""
url_video: ""
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_knit$set(root.dir = "~/Documents/training/class/data/train.json/")
```


```{r echo = FALSE, message=FALSE, warning=FALSE, eval = FALSE}
library(jsonlite)
library(ggplot2)
library(xgboost)
library(topicmodels)
library(dplyr)
library(tidyr)
library(tidytext)
library(purrr)
library(tibble)
library(stringr)
library(cognizer)
```

## **Learning Objectives**
- Package loading
- Data import
- Data clean up
- Data manipulation
- Piping
- Functions
- Data structures: vectors, lists, data frames
- Iteration
- Watson API services

## **Context**
To practice programming concepts, we will use a data set from a kaggle competition. [renthop.com](renthop.com) is a service for listing and searching for apartments. For each apartment, there is information about the bedrooms, bathrooms, pricing, and interest level, based on how many views a listing got when it was opened. In addition to numerical data, there is also text describing features of an apartment and images showing the decor, data that will require special treatment and provide new learning opportunities.

How much interest will a new rental listing on RentHop receive? This question motivates the competition, and will guide this assignment, which involves data import, clean up, manipulation, interacting with an api, assisted by functions and iteration.

<!-- the second part will cover predictive modeling that will involve taking all variables we have and using them to predict interest level to figure what makes for a good rental listing. -->

## **Data Import** 
Start by creating a project folder. Use the dataset link to obtain data; then move it to your project folder. Data is somewhat large, and has been compressed with zip algorithm. Assuming you moved downloaded data to your project folder, use the following R code to unzip it:
```
unzip("test.json.zip")
```
If the code does not work, do it manually with [7zip](http://www.7-zip.org/) on Windows or Archive Utility on Mac. Once you unzipped the file, you should see a test.json file in your project folder - make sure you do before proceeding. .json file is a popular storage format to transport data over the internet, and requires a special package and function to import it into R.
Run the following commands to get the required package and load it,
```
install.packages("jsonlite")
library(jsonlite)
```
and then run the following command to read the data into R,
```
train_raw <- fromJSON("train.json")
```
`fromJSON` and **jsonlite** are similar to `read_csv` and **readr** in that they import data into R, but address different formats.

## **Data Cleanup**
You should see a pretty large list. Explore it using these commands,
```
typeof(train_raw)
names(train_raw)
str(train_raw)
```
While `fromJSON` created a list, our data can be conveniently stored as a data frame, with each row being a listing and each column being some part of that listing. The only novel aspect of this data is how to handle **features** and **photos** because unlike standard column that have a single value per cell, apartment listings can have multiple features or photos, hence we need to somehow store a vector of multiple values in a single cell of a data frame.

Recall that data frames are made out of lists, and that lists can have lists inside of them and that components of a list can be vectors with multiple values of the same type. This means that a data frame can have a list column and allows us to store multiple features and photo urls in a single cell of a data frame.

Apply `as_tibble` and then structure of data to see that it is tabular. Next task is to fix types of columns. Most of the columns are list type, but should be integer or character. Write `for` loop code to modify the tibble by iterating over columns and applying `unlist` to all the columns but features and photos. Fill in the missing pieces of the `for` loop to carry this out:
```
for (sequence) {
  code
}
```

Next task is to combine features for a given apartment into a single string. For this we need to iterate over the cell values of the **features** column. Some values are empty and others contain one or more strings. If a value is empty, then we need to replace empty values with some default string like "nothing", otherwise collapse strings into a single string separated by a space. For this you will need to create a function that takes a list, loops over its components, replacing the value of a component if its length is 0 and applying `str_c` function from **stringr** package. Here is a logical skeleton:
```
func <- function(arguments) {
  for (sequence) {
    if (length is 0) replace with "nothing"
    collapse values into a single value
  }
  apply lower case transformation
  replace all symbols that are not english characters with empty space
  return value
}
```
After finishing this funciton, modify your data frame inside `mutate` with it. Next, we will examine what could be done with photo urls.

## **IBM Image Analysis**
In this part will further pratice function writing and iteration by downloading the photos using the urls and then sending them for object analysis to IBM. **photos** is a list column and each component is a vector of urls represented as strings. Install these packages before you go on:
```
install.packages("devtools")
library(devtools)
install_github("cbuscollaboratory/cognizer")
```


Next, we need to create a function that can take a vector of urls, go to the url to download a photo and store it locally on your hard drive, then send all the images together to IBM for object analysis. Here is the logical skeleton:

```
func <- function(arguments) {
  create a vector of names for the images
  download images from urls with names you created above
  send these images to IBM using image_classify function from cognizer package
  return results of IBM analysis 
}
```
I created a free account to try this for everyone. IBM services are what is called APIs - application programming interfaces - which require an api key to interact with. Once you have the key, you can store it as a variable and pass it to your function.
```{r echo = FALSE}
Sys.setenv(apikey = "e6984128a4b3210daffb8c37f015dd19b91c02c7")
```
The daily limit is 250 images, so everyone can send ten or so as an exercise. Use this code to get ten random urls and apply your function to it. Dont forget to assign a name to your results, so you can examine it further.
```
key <- "key goes here"
urls <- sample(unlist(df$photos), 10)
result <- func(urls, key)
```

The results you get from IBM is a complicated, nested list. The last part is to iterate over the results, pulling out the information about classes and algorithm's confidence scores and storing them in a list. To navigate through the list you will need to use a combination of names and positions of the components. Here is a logical skeleton of the problem:

```
output <- vector()
for (sequence) {
  output <- results$name[[i]]$name[[i]]
}
```
  
**Dont forget to put all your code into Rmarkdown and test that all of it runs without errors to produce an html file that contains your answers. Good luck!**   
