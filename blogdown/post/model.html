---
author: "Slava Nikitin"
date: "2017-04-03"
draft: false
tags: ["lecture"]
title: "Model"
summary: "Predictive models, accuracy diagnostics, xgboost"
math: false
output: 
  html_document:
    self_contained: true
---


<!-- BLOGDOWN-BODY-BEFORE

/BLOGDOWN-BODY-BEFORE -->

<p>Modeling of data patterns is a broad and deep topic that we can barely scratch in an intro class. We will concentrate on a couple types of models, aiming at prediction, on how to visualize models, and how to check predictive accuracy. The primary package we will use is <strong>xgboost</strong>.</p>
<div id="predictive-task" class="section level2">
<h2>Predictive task</h2>
<p>We consider a situation when we have a data frame of data, with <span class="math inline">\(n\)</span> rows and <span class="math inline">\(p\)</span> columns. We identify one column as a response variable and treat other variables as predictors. In addition, we distinguish between quantitative variables, for example, time or salary or rate of return, and categorical variables, like political affiliation or gender or loan approval status. The later distinction is necessary to make good choices about what type of R data is appropriate for different types of information. This may require some data preparation, but ultimately all information will be stored as <strong>double</strong>.</p>
<p>Given data and our choices about response / predictor variables, our task is to find a predictive function that takes values of predictors and returns a value close to the observed response variable. Here is how we formulate it mathematically.</p>
<p>Lets consider a row <span class="math inline">\(i \in \{1, 2, \dots, n\}\)</span> of a data frame, where one value is a response variable <span class="math inline">\(y_i \in \mathbb R\)</span> and remaining values are predictors <span class="math inline">\(x_{i1}, x_{i2}, \dots, x_{ip-1} \in \mathbb R ^ {p-1}\)</span>, hence accounting for the <span class="math inline">\(p\)</span> columns. We assume</p>
<ul>
<li>that <span class="math inline">\((y_i, x_{i1}, \dots, x_{ip-1})\)</span> have been sampled independently from a probability distribution <span class="math inline">\(P(y, x_{1}, \dots, x_{p-1})\)</span> that characterizes variation in each variable and dependencies among them, meaning data is representative,</li>
<li>that there is a function <span class="math inline">\(g(x_{1}, \dots, x_{p-1})\)</span> that provides a true description of how <span class="math inline">\(mean(y)\)</span> dependends on predictors, meaning there is a dependency of response on predictors.</li>
</ul>
<p>The first two assumptions are theoretical. To attempt to solve the problem, we also need practical assumptions that we are in control of and are primary components of what makes up a predictive model methodology: - that we can define a set of possible models, <span class="math inline">\(\{f(x_{1}, \dots, x_{p-1}, \theta) : \theta \in \Theta\ \subset \mathbb R\}\)</span>, such that there is at least one function in a set of possible functions that approximates the true relationship, <span class="math inline">\(f(x_{1}, \dots, x_{p-1}, \theta ^ *)\)</span>, meaning we can find an adequate model of the true relationship - that we can find a function to quantify overall predictive accuracy <span class="math inline">\(\sum_{i=1}^n L(y_i, f(x_{1}, \dots, x_{p-1}, \theta))\)</span>, called a loss function, meaning we can quantify discrepancy between truth and approximation - that we can obtain an estimation algorithm that searches through the possible values of <span class="math inline">\(\theta \in \Theta\)</span>, which modifies how response variable and predictors are related, so that we can maximize overall predictive accuracy, meaning we can solve <span class="math inline">\(\mathrm{argmin}_{\theta \in \Theta}\ L(\theta)\)</span> within computational, statistical, and organizational constraints - that we can come up with an interpretable metric of accuracy to characterize our model, meaning we can summarise how well it does, compare it to other models and communicate it to someone else</p>
<p>The above points are very general, and we need to specify some details to actually make this doable. First, we are going to use a set of functions and an estimation algorithm offered by package <code>xgboost</code>, which iteratively builds step-wise functions, called trees, and combines them by averaging their outputs. We could consider other function sets like planes, quadratics, exponential curves, but all of these are very restrained and it becomes harder to manually put these together in a multivariate case, so we will just rely on a very general set of function from <code>xgboost</code> that can approximate any of those and more.</p>
<p>Secondly, we are going to use mean squared error to quantify mean predictive error of a model that predicts a quantitative response, defined as <span class="math inline">\(\sqrt{\frac{1}{n}\sum_{i = 1}^n(y_i - f(x_{1}, \dots, x_{p-1}, \theta))^2}\)</span>, and misclassification rate for a categorical response, defined as <span class="math inline">\(\frac{1}{n}\sum_{i = 1}^n \delta(y_i \neq f(x_{1}, \dots, x_{p-1}, \theta))\)</span>, where <span class="math inline">\(\delta(condition)\)</span> is 1 if true and 0 if false.</p>
</div>
<div id="examples" class="section level2">
<h2>Examples</h2>
<p>To follow along run these commands:</p>
<pre><code>install.packages(&quot;xgboost&quot;)
library(xgboost)</code></pre>
<p>Lets look at some examples. <strong>mtcars</strong> serves as a good example:</p>
<pre class="r"><code>head(mtcars)</code></pre>
<pre><code>##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1</code></pre>
<p><strong>mpg</strong>, miles per gallon, is a quantitative variable while <strong>am</strong>, status of automatic transmission, is categorical. Suppose <strong>mpg</strong> is the response variable <span class="math inline">\(y\)</span>, then our task is to estimate an unknown mathematical function <span class="math inline">\(g(x_{cyl}, x_{disp},...,x_{carb})\)</span> that will take values of the other 10 variables in the table or similar values, and will output a predicted value of <strong>mpg</strong> <span class="math inline">\(\hat y\)</span> that is <em>close</em> to the makes of cars we see in the table, but also close to new, unseen car makes, so <span class="math inline">\(\hat y \approx y\)</span>. Outputs of <span class="math inline">\(f(\dots)\)</span> will be quantitative, continuous, positive; this is called a regression model. If we are successful, then we can predict <strong>mpg</strong> given values of the other variables.</p>
<p>This expresses our choices about response and predictors:</p>
<pre class="r"><code>response &lt;- &quot;mpg&quot;
preds &lt;- setdiff(names(mtcars), response)</code></pre>
<p><code>xgboost</code> has a special data structure <code>xgb.DMatrix</code> to which we pass our variables:</p>
<pre class="r"><code>data &lt;- xgb.DMatrix(data = as.matrix(mtcars[preds]), label = mtcars[[response]])</code></pre>
<p>Next, we estimate a predictive model. First we need to specify what kind of predictive problem is it, regression or classification?</p>
<pre class="r"><code>params &lt;- list(objective = &quot;reg:linear&quot;, max.depth = 2)</code></pre>
<p>Here <code>&quot;reg:linear&quot;</code> means regression, so predicting a quantitative response variable. We will see other <em>objective</em> settings when doing classification. Then we can run the estimation algorithm to obtain a candidate model:</p>
<pre class="r"><code>model &lt;- xgboost(data, nrounds = 15, params = params)</code></pre>
<pre><code>## [0]  train-rmse:14.931315
## [1]  train-rmse:10.956807
## [2]  train-rmse:8.086657
## [3]  train-rmse:6.014954
## [4]  train-rmse:4.524509
## [5]  train-rmse:3.452733
## [6]  train-rmse:2.699348
## [7]  train-rmse:2.160562
## [8]  train-rmse:1.783578
## [9]  train-rmse:1.530202
## [10] train-rmse:1.352115
## [11] train-rmse:1.221489
## [12] train-rmse:1.125132
## [13] train-rmse:1.050768
## [14] train-rmse:0.993811</code></pre>
<p>So, we have a model. How well does it do? This is a problem of diagnostics. We can use quantitative and visual methods to establish the predictive accuracy of our model. The important thing is to do model building and checking on different parts of a dataset, sometimes called train and test data. This way information for modeling and evaluating its accuracy is distinct. This is important for reducing or eliminating the bias that arises when you build a solution and then test it against the same data. Given that the solution is obtained by searching for a function that closely resembles patterns in the data, it should not be surprising if such a function shows high predictive accuracy - it was meant to.</p>
<p>Luckily, <code>xgboost</code> provides a special function to handle data partitions automatically. The technique is called <strong>cross-validation</strong>. Data gets partitioned into non-overlapping subsets. A model is found for all but one subset and its accuracy is quantified against the held-out subset. This allows to control for bias in results. Then we repeat the same process by holding out a different dataset and estimating a model on the remaning. This is repeated until each dataset has served as a test dataset.</p>
<p>Here is how you do it in <code>xgboost</code>:</p>
<pre class="r"><code>model_check &lt;- xgb.cv(data = data, nrounds = 15, params = params, 
                      nfold = 5, prediction = TRUE, verbose = FALSE)</code></pre>
<pre><code>## $dt
##     train.rmse.mean train.rmse.std test.rmse.mean test.rmse.std
##  1:       14.993933       0.291984      14.822759      1.634960
##  2:       11.068695       0.219158      11.123965      1.554061
##  3:        8.231948       0.148299       8.329076      1.521883
##  4:        6.173751       0.101293       6.478337      1.500839
##  5:        4.670877       0.064149       5.173599      1.504159
##  6:        3.582215       0.042977       4.243949      1.516869
##  7:        2.792007       0.031579       3.678892      1.448387
##  8:        2.221686       0.024015       3.365655      1.319669
##  9:        1.815382       0.023632       3.168318      1.177537
## 10:        1.524795       0.022420       3.051389      1.066124
## 11:        1.315182       0.026904       2.997304      0.962810
## 12:        1.162248       0.029580       2.964006      0.919975
## 13:        1.045998       0.037697       2.946803      0.853121
## 14:        0.957839       0.046413       2.954466      0.806175
## 15:        0.882804       0.035988       2.977791      0.808784
## 
## $pred
##  [1] 20.16540 21.20982 25.94812 18.13794 16.48970 20.21816 15.24580
##  [8] 26.06040 21.14948 16.87186 20.08516 17.39670 15.75836 17.39670
## [15] 14.96215 14.96215 11.47685 27.09625 30.94643 30.16395 22.97746
## [22] 17.90004 17.74608 14.65228 15.80265 31.88300 22.31139 20.98132
## [29] 16.32927 18.55604 13.80461 20.85459</code></pre>
<p>Output of <code>xgb.cv</code> contains summary statistics on train and test data, and also predictions on the hold-out data. In addition to summary statistics, we can also do a few visualizations of the full pattern of predictive accuracy.</p>
<pre class="r"><code>plot.ts(as.data.frame(model_check$dt)[,c(1, 3)], plot.type = &quot;single&quot;)</code></pre>
<p><img src="#####../content/post/model_files/figure-html/unnamed-chunk-9-1.png" width="672" /> The plot above shows mean predictive error in the model over training iterations. The gap between train and test shows the slight bias towards predicting train rather than test data. A good model should have both values low and not too far from each other.</p>
<p>We can also examine residuals - observed minus predicted values - by plotting their histogram:</p>
<pre class="r"><code>hist(mtcars$mpg - model_check$pred)</code></pre>
<p><img src="#####../content/post/model_files/figure-html/unnamed-chunk-10-1.png" width="672" /> A good model should have this centered around 0.</p>
<p>We can also check for a relationship between observed and predicted values. For a good model, these should be highly correlated:</p>
<pre class="r"><code>plot(mtcars$mpg, model_check$pred)</code></pre>
<p><img src="#####../content/post/model_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>All the summary statistics and plots look pretty good. We can use the model we build to generate predictions, for old or new inputs. We would only need to convert data with the predictors into a <code>xgb.DMatrix</code>. Here is an example using the predict function from base R on the full dataset:</p>
<pre class="r"><code>predict(model, data)</code></pre>
<pre><code>##  [1] 20.28938 20.28938 22.15593 20.14867 17.01779 18.76633 14.85491
##  [8] 23.08412 22.15593 18.94006 18.94006 16.05549 16.05549 16.05549
## [15] 10.59728 10.59728 14.73103 30.88917 30.01993 32.20821 22.15593
## [22] 16.50219 16.53521 14.69125 17.01779 28.23297 25.88072 28.94273
## [29] 15.72165 20.09434 14.69125 22.15593</code></pre>
<p>Similarly, if we used <strong>am</strong> as the response variable, which is categorical, the task is also to find a predictive function, with the only difference from <strong>mpg</strong> is that the output should be categorical, perhapse 1 for automatic and 0 manual, and no other possible values. By outputing only a few categorical values is the reason why such a predictive function is often called a classifier.</p>
<p>We can use <code>xgboost</code> to solve this problem, too. Lets first setup our variables and data.</p>
<pre class="r"><code>response &lt;- &quot;am&quot;
preds &lt;- setdiff(names(mtcars), response)

data &lt;- xgb.DMatrix(data = as.matrix(mtcars[preds]), label = mtcars[[response]])</code></pre>
<p>The classification problem is different and requires a different way to capture a discrepancy between a candidate model and the underlying true function. Here we use <code>&quot;binary:logistic&quot;</code>:</p>
<pre class="r"><code>params &lt;- list(objective = &quot;binary:logistic&quot;, max.depth = 2)
model &lt;- xgboost(data, nrounds = 15, params = params)</code></pre>
<pre><code>## [0]  train-error:0.093750
## [1]  train-error:0.062500
## [2]  train-error:0.031250
## [3]  train-error:0.031250
## [4]  train-error:0.031250
## [5]  train-error:0.031250
## [6]  train-error:0.031250
## [7]  train-error:0.031250
## [8]  train-error:0.031250
## [9]  train-error:0.031250
## [10] train-error:0.000000
## [11] train-error:0.031250
## [12] train-error:0.000000
## [13] train-error:0.031250
## [14] train-error:0.031250</code></pre>
<p>Given our model, we once again ask how well it can do predict under new conditions.</p>
<pre class="r"><code>model_check &lt;- xgb.cv(data = data, nrounds = 15, params = params, 
                      nfold = 5, prediction = TRUE, verbose = FALSE)</code></pre>
<pre><code>## $dt
##     train.error.mean train.error.std test.error.mean test.error.std
##  1:         0.070792        0.034027        0.222857       0.184501
##  2:         0.046792        0.016893        0.194286       0.209372
##  3:         0.039100        0.001332        0.194286       0.209372
##  4:         0.039100        0.001332        0.194286       0.209372
##  5:         0.031100        0.017429        0.194286       0.209372
##  6:         0.031100        0.017429        0.194286       0.209372
##  7:         0.023407        0.021402        0.194286       0.209372
##  8:         0.023407        0.021402        0.194286       0.209372
##  9:         0.023407        0.021402        0.194286       0.209372
## 10:         0.031100        0.017429        0.194286       0.209372
## 11:         0.023407        0.021402        0.194286       0.209372
## 12:         0.023100        0.021113        0.194286       0.209372
## 13:         0.015407        0.021123        0.194286       0.209372
## 14:         0.023100        0.021113        0.194286       0.209372
## 15:         0.015407        0.021123        0.194286       0.209372
## 
## $pred
##  [1] 0.93444717 0.93444717 0.86359012 0.03390779 0.06941897 0.04334350
##  [7] 0.06941897 0.33191431 0.93641293 0.70334697 0.70334697 0.09114783
## [13] 0.05321305 0.06589319 0.03390779 0.05321305 0.09114783 0.86729366
## [19] 0.86680818 0.83597845 0.55232960 0.04659716 0.05118420 0.12264583
## [25] 0.07874634 0.86729366 0.93522042 0.87377137 0.38232031 0.92300272
## [31] 0.09084942 0.86680818</code></pre>
<pre class="r"><code>plot.ts(as.data.frame(model_check$dt)[,c(1, 3)], plot.type = &quot;single&quot;)</code></pre>
<p><img src="#####../content/post/model_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>When dealing with a classification problem, instead of residuals, it is typicaly to see which classes a model gets well, and how often, using what is called a confusion matrix. Basically, we count how many correct and incorrect predictions are made and put them into a table. Note that predictions from the model are actually probabilities, and to get a class prediction we need to threshold a probability. It is typical to use .5.</p>
<pre class="r"><code>table(pred = as.integer(model_check$pred &gt; .5), obs = mtcars$am)</code></pre>
<pre><code>##     obs
## pred  0  1
##    0 15  2
##    1  4 11</code></pre>
<p>We the model can discriminate between automatic and manual pretty well, only confusing the two sometimes.</p>
<p>This example showed a two-class prediction problem, but <code>xgboost</code> can handle multi-class problem, too. We just need to adjust our parameters by setting objective argument by to <code>&quot;multi:softmax&quot;</code> and <code>num_class</code> to number of classes we are dealing with. Lastly, the classes should be represented with integers, starting from 0 and going to <code>num_class - 1</code>.</p>
</div>


<!-- BLOGDOWN-HEAD




/BLOGDOWN-HEAD -->
