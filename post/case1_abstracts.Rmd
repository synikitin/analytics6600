---
title: "Case Study 1"
summary: "Conference Proceedings Analysis"
author: "Slava Nikitin"
date: "2018-01-22"
draft: false
tags: ["lecture"]
output: 
  html_document:
    self_contained: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Conference Proceedings Analysis

### Project

- **Background**  
  A health care company XYZ LLC hosts conferences on novel variants of cancer and technologies meant to detect it early. Scientists in charge of it are responsible for preparing a summary presentation of past and present research.
- **Objectives**  
  Understand origins of authors, co-author network, novel variants of cancer, clusters of abstracts based on similar topics, trends across the last decade in most prominent research topics.
- **Benefits**  
  Maintain reputation of thought leadership in the area of cancer research, advance knowledge by submitting a paper into a proceedings journal based on the trends, come up with a repeatable, automated process, potentially **reuse** the process as a service to meet the demand of understanding ongoing trends in technology.
- **Data Specification**  
  Data from prior conferences in .docx, .pdf, .xlsx, .csv will be provided to support the work.
- **Solution**  
  Visuals for the 5 objectives will be provided with appropriate resolution as .jpeg images. Code underlying all the data preparation and analysis will be provided, too. 


### People
- A project manager and a data scientist from Awesome Analytics.
- A cancer scientist from the client to guide the design and aesthetic features of the visuals.   

### Process
- **Phases**  
  Emphasize, Define, Ideate, Prototype, Test, Implement
- **Communication**  
  Weekly meetings will take place to collect feedback.
- **Security Classification**  
  Data is public.
  
### Issues, Ideas and Principles
- **Purpose**  
  Understanding of conference authors and research trends, not simple data summaries or prediction or technology. Maybe a technology - a program rerun each conference.
- **Confidentiality**  
  Transmission of public data does not pose much issue: email is a valid method. Things get more complicated with private or classified data.
- **Data Formats**  
  Text data can be stored on disk using many formats, each requiring its own parsing methods to  convert text into a common representation.
- **Data Encoding**  
  Technical abstracts are submitted from around the world. Text in abstracts contains many special symbols and letters from other alphabets. These are handled by an expanded character set, called UTF-8, but may require special handling when cleaning text.
- **Data Collection**   
  Generated by humans with a non-standard process which creates much nuisance variation in data, say how names and countries are written down, that complicates data preparation.
- **Sources of variation**  
  Key idea in dealing with much data, including messy text, is to think of it in terms of variable components that combine together to make the observed data. 
- **Automated Tools vs Manual Rules**      
  Data with nuisance variation breaks assumptions of automated tools and often requires creation of manually crafted rules, one by one, through trial and error. Regular expression and dictionaries are key technologies. This, of course, slows down work quite a bit.
- **Simple versus Complex Transformations**  
The questions of author origins and cooperation can be obtained through straightforward manipulations of the provided data. However, finding novel variants of cancer, clustering abstracts by topic or trending research topics over time is not as clear-cut. The later set of problems requires statistical models to give us algorithms that can output probabilities, topics, and clusters. 

  
###  Relevant R packages 
- **Data**  
  state.abb, mapdata
- **Import**  
  readxl, readr, pdftools
- **Tidy**  
  tidytext, tidygraph
- **Transform**  
  xml2, dplyr, stringr
- **Visualize**  
  ggplot2, ggrepel
- **Model**  
  topicmodels
- **Communicate**  
  Rmarkdown
  









